{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3aa0add",
   "metadata": {},
   "source": [
    "# STA410 Week 11 Programming Assignment (10 points)\n",
    "\n",
    "Welcome.\n",
    "\n",
    "## Rules\n",
    "\n",
    "\n",
    "0. **This is a paired or individual assignment.** Specific code solutions submitted for these assignments must be created either individually or in the context of a paired effort: ***group efforts of three or more are students are not allowed.*** Please seek homework partners in-person or on the course discussion board on piazza. **Paired students each separately submit their (common) work, including (agreeing) contribution of work statements for each problem.**\n",
    "  \n",
    "   > Students choosing to work individually must work in accordance with the [University Student Academic Integrity values](https://www.artsci.utoronto.ca/current/academic-advising-and-support/student-academic-integrity)  of \"honesty, trust, fairness, respect, responsibility and courage.\" Students working in pairs may share work without restriction within their pair, but must otherwise work in accordance with the [University Student Academic Integrity values](https://www.artsci.utoronto.ca/current/academic-advising-and-support/student-academic-integrity) noted above. ***Getting and sharing \"hints\" from other classmates is allowed; but, the eventual code creation work and submission must be your own individual or paired creation.***\n",
    "   \n",
    "   \n",
    "1. **Do not delete or replace cells**: this erases `cell ids` upon which automated code tests are based.\n",
    "\n",
    "    - ***If you accidentally delete a required cell*** try \"Edit > Undo Delete Cells\" in the notebook editor; otherwise, redownload the notebook (so it has the correct required `cells ids`) and repopulate it with your answers (assuming you don't overwrite them when you redownload the notebook).\n",
    "\n",
    "   - ***You may add cells for scratch work*** but if required answers are not submitted through the provided cells where the answers are requested your answers may not be marked.\n",
    "\n",
    "  > You may check if `cell ids` are present and working by running the following command in a cell \n",
    "  >\n",
    "  > `! grep '\"id\":' <path/to/notebook>.ipynb`\n",
    "  >\n",
    "  > and making sure the `cell ids` **do not change** when you save your notebook.\n",
    "  >\n",
    "  >> ***If you are working in any environment other than*** [UofT JupyterLab](https://jupyter.utoronto.ca/hub/user-redirect/git-pull?repo=https://github.com/pointOfive/sta410hw0&branch=master&urlpath=/lab/tree/sta410hw0), [UofT JupyterHub](https://jupyter.utoronto.ca/hub/user-redirect/git-pull?repo=https://github.com/pointOfive/sta410hw0&branch=master), or [Google Colab](https://colab.research.google.com/github/pointOfive/sta410hw0/blob/master/sta410hw0.ipynb), your system must meet the following versioning requirements \n",
    "   >>\n",
    "   >>   - [notebook format >=4.5](https://github.com/jupyterlab/jupyterlab/issues/9729) \n",
    "   >>   - jupyter [notebook](https://jupyter.org/install#jupyter-notebook) version [>=6.2](https://jupyter-notebook.readthedocs.io/en/stable/) for \"classic\" notebooks served by [jupyterhub](https://jupyterhub.readthedocs.io/en/stable/quickstart.html)\n",
    "   >>   - [jupyterlab](https://jupyter.org/install) version [>=3.0.13](https://github.com/jupyterlab/jupyterlab/releases/tag/v3.0.13) for \"jupyterlab\" notebooks  \n",
    "   >>    \n",
    "   >> otherwise `cell ids` will not be supported and you will not get any credit for your submitted homework.  \n",
    "      \n",
    "2. **No cells may have any runtime errors** because this causes subsequent automated code tests to fail and you will not get marks for tests which fail because of previous runtime errors. \n",
    "\n",
    "  - Run time errors include, e.g., unassigned variables, mismatched parentheses, and any code which does not work when the notebook cells are sequentially run, even if it was provided for you as part of the starter code. ***It is best to restart and re-run the cells in your notebook to ensure there are no runtime errors before submitting your work.***\n",
    "    \n",
    "  - The `try`-`except` block syntax catches runtime errors and transforms them into `exceptions` which will not cause subsequent automated code tests to fail.  \n",
    "\n",
    "\n",
    "3. **No jupyter shortcut commands** such as `! python script.py 10` or `%%timeit` may be included in the final submission as they will cause subsequent automated code tests to fail.\n",
    "\n",
    "  - ***Comment out ALL jupyter shortcut commands***, e.g., `# ! python script.py 10` or `# %%timeit` in submitted notebooks.\n",
    "\n",
    "\n",
    "4. **Python library imports are limited** to only libraries imported in the starter code and the [standard python modules](https://docs.python.org/3/py-modindex.html). Importing additional libraries will cause subsequent automated code tests to fail.\n",
    "\n",
    "  > Unless a problem instructs differently, you may use any functions available from the libraries imported in the starter code; otherwise, you are expected to create your own Python functionality based on the Python stdlib (standard libary, i.e., base Python and standard Python modules).\n",
    "\n",
    "\n",
    "5. You are welcome and encouraged to adapt code you find available online into your notebook; however, if you do so you must provide a link to the utilized resource. ***If failure to cite such references is identified and confirmed, your mark will be immediately reduced to 0.***  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e256d28",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d48de6e",
   "metadata": {},
   "source": [
    "# Problem 0 (required)\n",
    "\n",
    "Are you working with a partner to complete this assignment?  \n",
    "- If not, assign  the value of `None` into the variable `Partner`.\n",
    "- If so, assign the name of the person you worked with into the variable `Partner`.\n",
    "    - Format the name as `\"<First Name> <Last Name>\"` as a `str` type, e.g., \"Scott Schwartz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78081a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Required: only worth points when not completed, in which case, you'll lose points\n",
    "Partner = #None\n",
    "# This cell will produce a runtime error until you assign a value to this variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482b3e6",
   "metadata": {},
   "source": [
    "What was your contribution in completing the code for this assignments problems? Assign one of the following into each of the `Problem_X` variables below.\n",
    "\n",
    "- `\"I worked alone\"`\n",
    "- `\"I contributed more than my partner\"`\n",
    "- `\"My partner and I contributed equally\"`\n",
    "- `\"I contributed less than my partner\"`\n",
    "- `\"I did not contribute\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589b6a8",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Required: only worth points when not completed, in which case, you'll lose points\n",
    "Problem_1 = #\"I worked alone\"\n",
    "Problem_2 = #\"I worked alone\"\n",
    "# This cell will produce a runtime error until you assign a value to this variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db8897",
   "metadata": {},
   "source": [
    "# Problem 1 (5 points)\n",
    "\n",
    "# IMPORTANT/CRITICAL: this assignment must be completed as TWO separate notebooks. \n",
    "\n",
    "# *DO NOT PUT PyMC OR ANY OTHER CODE IN THIS NOTEBOOK*\n",
    "## Add and execute your code in another notebook named `PyMC_intro.ipynb`\n",
    "\n",
    "### *PyMC is currently undergoing many updates and is (unfortunately) thus not currently compatible with MarkUs and the UofT JuypterHub resources; thus, for versioning and package availability compatibility you MUST complete this assignment with google colab or a local anaconda installation. For a local conda installation use the following envirionment creation and activation.*\n",
    "\n",
    "```\n",
    "conda create -c conda-forge -n pymc_env \"pymc>=4\"\n",
    "conda activate pymc_env\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "1. Go to https://www.pymc.io/welcome.html <!--https://docs.pymc.io/en/v3/-->\n",
    "2. Scroll down a little bit and click the \"Beginner guide (if you do not know Bayesian modeling)\" link, which will take you [here](https://www.pymc.io/projects/docs/en/latest/learn/core_notebooks/pymc_overview.html)\n",
    "2. Complete the tutorial by copying and running the necessary code as cells into A DIFFERENT NOTEBOOK titled `PyMC_intro.ipynb`.\n",
    "3. Answer the following questions in this notebook\n",
    "\n",
    "# ONLY PROVIDE ABCD ANSWERS here: don't include PyMC code here\n",
    "\n",
    "***The following 25 questions are each worth 0.2 points.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a34af9",
   "metadata": {
    "deletable": false
   },
   "source": [
    "0. **p1q0.** What prior distribuiton is used for $\\sigma^2$ in the \"Linear Regression Example\"?\n",
    "\n",
    "    1. A normal distribution\n",
    "    2. A standard normal distribution\n",
    "    3. The positive half of a standard normal distribution\n",
    "    4. None of the above\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "1. **p1q1.** What is the ***conjuage*** prior distribuiton for $\\sigma^2$ in Bayesian Linear Regression as given [here](https://en.wikipedia.org/wiki/Bayesian_linear_regression)?\n",
    "\n",
    "    1. Gamma distribution\n",
    "    2. Inverse-Gamma distribution\n",
    "    3. Normal distribution\n",
    "    4. None of the above\n",
    "    \n",
    "\n",
    "<br>\n",
    "\n",
    "2. **p1q2**. What explains the answers to the previous two questions?   \n",
    "\n",
    "    1. Only a ***conjuage*** prior distribuiton has the right support for a parameter\n",
    "    2. The PyMC example uses the ***conjuage*** prior distribuiton as is expected\n",
    "    3. The fact that a prior is subjective and is the choice of the designer of the model\n",
    "    4. None of the above\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **p1q3**. What was the second sampler that was used in the \"Linear Regression Example\"?\n",
    "\n",
    "    1. The Slice sampler\n",
    "    2. The NUTS sampler\n",
    "    3. The MCMC Metropolis-Hastings sampler\n",
    "    4. None of the above\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "4. **p1q4**. What is the NUTS sampler? \n",
    "\n",
    "    1. The No U-Turn Sampler, a self-tuning variant of Hamiltonian Monte Carlo\n",
    "    2. An MCMC technique which samples uniformly under a density from alternating horizontal and vertical lines underneath the density\n",
    "    3. An rejection sampling method based on uniform sampling\n",
    "    4. None of the above\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "5. **p1q5**. How many unique parameters are there in the \"Linear Regression Example\" (not counting `mu` which is deterministically implied by a subset of the other parameters)?\n",
    "\n",
    "    1. 1\n",
    "    2. 2\n",
    "    3. 3\n",
    "    4. 4\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "6. **p1q6**. The posterior analysis for the \"Linear Regression Example\" was based on which sampling technique?\n",
    "\n",
    "    1. The Slice sampler\n",
    "    2. The NUTS sampler\n",
    "    3. The MCMC Metropolis-Hastings sampler\n",
    "    4. None of the above\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "7. **p1q7**. The NUTS sampler provided two chains each listed as 2000/2000, while the Slice sampler provided two chains each listed as 6000/6000, even though the number of specified samples was 5000.  What explains this?\n",
    "\n",
    "    1. Two chains of 1000 makes 2000 and 2000 of those samples worked\n",
    "    2. It appears the default \"burn-in\" is 1000 samples\n",
    "    3. The 6000/6000 is just a standard PyMC status printout when creating chains\n",
    "    4. None of the above\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "8. **p1q8**. Why are there two colors of chains for the `beta` plot in the posterior analysis for the \"Linear Regression Example\", but not for the other parameter plots? \n",
    "\n",
    "    1. Because there are two chains created by the sampler\n",
    "    2. Because in the other plots dashed and solid lines are used instead\n",
    "    3. Because the beta parameter is a two-dimensional parameter\n",
    "    4. None of the above\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "9. **p1q9**. The ***effective sample size*** depends on the ***autocorrelation*** in the MCMC chains. \n",
    "\n",
    "    > For more details, see the [STAN manual](https://mc-stan.org/docs/reference-manual/effective-sample-size.html), based on [this manuscript](https://arxiv.org/pdf/1903.08008.pdf), and the [implementation details](https://oriolabril.github.io/arviz/api/generated/arviz.ess.html) in the [arvis source code](https://oriolabril.github.io/arviz/_modules/arviz/stats/diagnostics.html#ess); however, suffice it say that (a) positive autocorrelations decrease ***effective sample size*** while negative autocorrelations increase ***effective sample size***, and (b) the NUTS HMC sampler can produce negative autocorrelations.\n",
    "    \n",
    "    The posterior analysis for the \"Linear Regression Example\" shows two ***effective sample size*** estimations. The `ess_bulk` is just based on the autocorrelations in the chains, while the `ess_tail` is based on the autocorrelation in the chains after they're converted into binary sequences that estimate a tail quantile (like 5% or 95%). What do these two ***effective sample size*** mean relative to the 2000 actual samples produced by the NUTS sampler?\n",
    "\n",
    "    1. The `ess_bulk` ***effective sample size*** estimate for estimating the parameter is greater than the actual number of NUTS samples\n",
    "    2. The `ess_bulk` ***effective sample size*** estimate for estimating a quantile in the tail of the  distribution of the parameter is less than the number of NUTS samples\n",
    "    3. Both A and B above\n",
    "    4. None of the above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d621fed",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 2 points (0.2 points each) [format: `str` either \"A\" or \"B\" or \"C\" or \"D\" based on the choices above]\n",
    "p1q0 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q1 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q2 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q3 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q4 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q5 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q6 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q7 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q8 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q9 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "# Uncomment the above and keep only either \"A\" or \"B\" or \"C\" or \"D\"\n",
    "\n",
    "# This cell will produce a runtime error until the variables `p1q0`-`p1q9` are assigned values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270c3f4",
   "metadata": {},
   "source": [
    "<!-- FIXED SO THIS IS NO LONGER NEEDED!\n",
    "***To continue runing the tutorial, change the following:***\n",
    "```python\n",
    "#import pytensor.tensor as at\n",
    "import aesara\n",
    "import aesara.tensor as at\n",
    "```\n",
    "Updates to PyMC 4 are ongoing and it appears `pytensor` is not yet stable relative to `aesara` on google colab.\n",
    "\n",
    "- As you continue to run the code you will likely see some warnings, which can be safely ignored.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f39902",
   "metadata": {},
   "source": [
    "10. **p1q10**. In \"Case study 1\", what kind of regularization is used?\n",
    "\n",
    "    1. Ridge Shrinkage \n",
    "    2. Lasso Shrinkage\n",
    "    3. Horseshoe Shrinkage\n",
    "    4. None of the above\n",
    "\n",
    "<br>\n",
    "\n",
    "11. **p1q11**. In \"Case study 1\", why is $\\beta_i\\sim N(0, \\tau^2 {\\tilde \\lambda}_i^2)$ reparamterized as $\\beta_i = z_i \\tau {\\tilde \\lambda}_i$ with $z_i \\sim N(0, 1)$?\n",
    "\n",
    "    1. To allow the NUTS sampler to sample the $\\beta_i$ more efficiently\n",
    "    2. So that the density of $\\beta_i$ has high \"energy curvature\" that changes rapidly to reflect changes in $\\tau$ and ${\\tilde \\lambda}_i$\n",
    "    3. Because the two specifications are not mathematically equivalent and the former is preferred\n",
    "    4. None of the above\n",
    "\n",
    "<br>\n",
    "\n",
    "12. **p1q12**. Run the following different model for \"Case study 1\".\n",
    "\n",
    "    > ```python\n",
    "    > with pm.Model(coords={\"predictors\": X.columns.values}) as test_score_model:\n",
    "        sigma = pm.HalfNormal(\"sigma\", 25)\n",
    "        tau = pm.HalfStudentT(\"tau\", 2, D0 / (D - D0) * sigma / np.sqrt(N))\n",
    "        z = pm.Normal(\"z\", 0.0, 1.0, dims=\"predictors\")\n",
    "        beta = pm.Deterministic(\"beta\", z * tau, dims=\"predictors\")\n",
    "        beta0 = pm.Normal(\"beta0\", 100, 25.0)\n",
    "        scores = pm.Normal(\"scores\", beta0 + at.dot(X.values, beta), sigma, observed=y.values)\n",
    "    >\n",
    "    > with test_score_model:\n",
    "    >    idata = pm.sample(1000, tune=2000, random_seed=42)\n",
    "    >      \n",
    "    > az.plot_forest(idata, var_names=[\"beta\"], combined=True, hdi_prob=0.95, r_hat=True);    \n",
    "    > ```\n",
    "  \n",
    "    Which of the choices below which best describes the difference between the coefficient estimaiton of this model versus that of the originally specified model? \n",
    "   \n",
    "    1. There is now slightly more shrinkage towards zero on the two largest magnitude coefficients, and slightly less shrinkage towards zero on the smallest magnitude coefficients\n",
    "    2. There is more shrinkage towards zero for all of its coefficient estimates\n",
    "    3. There is less shrinkage towards zero for all of its coefficient estimates\n",
    "    4. None of the above\n",
    "\n",
    "<br>\n",
    "\n",
    "13. **p1q13**. Convert back to the original code, and then convert back to the initial parameterization of the model for \"Case study 1\" as follows. \n",
    "\n",
    "    > ```python\n",
    "    > # make these changes to the code above\n",
    "    > #z = pm.Normal(\"z\", 0.0, 1.0, dims=\"predictors\")\n",
    "    > beta = pm.Normal(\"beta\", 0.0, tau * lam * at.sqrt(c2 / (c2 + tau**2 * lam**2))**2, dims=\"predictors\")\n",
    "    > # then run\n",
    "    > ```\n",
    "\n",
    "    Which of the choices below best describes the change in the energy distributions?\n",
    "    \n",
    "    1. The energy distributions are now completely nonoverlapping\n",
    "    2. The energy distributions do still overlap, but they are now quite different from each other\n",
    "    3. The energy distributions cannot be calculated because of how many divergences there are\n",
    "    4. The energy distributions are still very similar  \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "14. **p1q14**. What is the problem caused by changing the model back to the original parameterization for \"Case study 1\"?\n",
    "\n",
    "    1. The energy distributions no longer nicely overlap\n",
    "    2. It requires the use of an additional $z_i$ variable which is unnecessary\n",
    "    3. The divergences mean the algorithm is not as computationally efficient as can be seen from `ess_bulk` and `ess_tail` in `az.summary(idata, round_to=2)`\n",
    "    4. No problem is caused by changing back to the original parameterization\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "15. **p1q15**. Run the following code for the \"Case study 2\" model.\n",
    "\n",
    "    > ```python\n",
    "    > import logging\n",
    "    > _log = logging.getLogger(\"pymc\")\n",
    "    > _log.setLevel(logging.INFO)\n",
    "    > with disaster_model:\n",
    "    >   idata = pm.sample(10000)\n",
    "    > ```\n",
    "    \n",
    "    What information provided when this is run?\n",
    "    \n",
    "    1. That Metropolis is used for `switchpoint` and `disasters_missing` and NUTS for `early_rate` and `late_rate`\n",
    "    2. That there were many divergences during model fitting indicating problematic sampling\n",
    "    3. That the effective sample size is lower than the targeted 80% level and to consider adjusting the `target_accept` parameter\n",
    "    4. That the energy transition distribution is significantly different than the final observed marginal energy distribution\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "16. **p1q16**. NUTS is an HMC method and therefore depends on computing gradients of continuous functions for the purposes of discretely approximating the evolution of a system of differential equations. Why are `switchpoint` and `disasters_missing` assigned as a Metropolis step while `early_rate` and `late_rate` are assigned as a NUTS step?\n",
    "\n",
    "    1. Because `early_rate` and `late_rate` are continuous variables, so they can be updated with NUTS\n",
    "    2. Because `switchpoint` and `disasters_missing` are discrete varibles, so they cannot be updated with NUTS\n",
    "    3. Both A and B\n",
    "    4. Neither A nor B\n",
    "    \n",
    "    \n",
    "<br>\n",
    "    \n",
    "17. **p1q17**. What is the function of the `disasters_missing` object?\n",
    "\n",
    "    1. It is missing data which can be treated as a parameter having a posterior distribution\n",
    "    2. It is necessary in order to specify an underlying unobserved Poisson process in PyMC\n",
    "    3. It specifies extrapolation targets for predictions projecting forward in time\n",
    "    4. None of the above\n",
    "\n",
    "    \n",
    "<br>\n",
    "\n",
    "18. **p1q18**. Which of the following are the posterior distributions of the \"earlier\" and \"later\" disaster rate parameters for the change point model?\n",
    "\n",
    "    1. $p(e|data) = \\sum_s p(e|data,s)p(s|data)$ and $p(l|data) = \\sum_s p(l|data,s)p(s|data)$\n",
    "    2. $p(e|data) = \\sum_s p(e|data,s)p(s)$ and $p(l|data) = \\sum_s p(l|data,s)p(s)$\n",
    "    3. $e \\sim \\exp(1)$ and $l \\sim \\exp(1)$\n",
    "    4. $p(e|data,s)$ and $p(l|data,s)$\n",
    "\n",
    "    \n",
    "<br>\n",
    "\n",
    "19. **p1q19**. How does the Metropolis step for `switchpoint` and `disasters_missing` work?\n",
    "\n",
    "    1. An integer year and count, respectively, is sampled from some proposal distribution that may or may not depend on the current values of the other parameters in the model\n",
    "    2. The current value of these parameters is retained or replaced with the proposal according to the chance prescribed by the Metropolis-Hastings accept-reject probabilities\n",
    "    3. A and then B in that order\n",
    "    4. The multidimensionl vector of `switchpoint` and `disasters_missing` parameters are evolved continuously according to the Hamiltonian dynamics defined as a system of differntial equations satisfying the law of conservation of energy \n",
    "\n",
    "    \n",
    "<br>\n",
    "\n",
    "20. **p1q20**. A vector of continuous variables can be evolved according to the Hamiltonian dynamics defined as a system of differential equations satisfying the law of conservation of energy. When working well, this leads to proposals with a high acceptatnce rate and sampling (even for very high dimensional vectors) that efficiently explorores the typical (representative) set of a target distribution.  Unfortunately it is unclear how to make similarly efficient proposals for high dimensional vectors of discrete variables. Thus, instead, small subsets of discrete variables may be randomly purturbed to create proposed updates to these variables which are then accepted or rejected using Metropolis-Hastings. What are good analogies to these two processes?\n",
    "\n",
    "    1. Continuous parameters can all be updated with a single proposal, like how gradient descent can make changes to all parameter coordinates at once. \n",
    "    2. Discrete parameters must be sequentially updated in small blocks, similar to how Gauss-Seidel only updates one parameter coordinate at a time. \n",
    "    3. Both A and B together describe the differences between these two processes, which shows that sampling from high dimensional discrete variables presents a bottleneck for Markov Chain Monte Carlo methods.\n",
    "    4. There is no difference because NUTS is an HMC method and also uses a Metropolis-Hastings proposal.\n",
    "    \n",
    "<br>\n",
    "\n",
    "\n",
    "21. **p1q21**. Does the NUTS HMC method actually use a Metropolis-Hastings accept-reject step? \n",
    "\n",
    "    1. No, HMC is a Gibbs sampling method unlike Metropolis-Hastings\n",
    "    2. No, HMC avoids a Metropolis-Hastings step which is why an algorithm like NUTS is so efficient\n",
    "    3. Yes, and it is therefore equivalent to a Metropolis-Hastings step that uses noisy perturbtion proposals \n",
    "    4. Yes, it just uses a proposal distribution which efficiently explores the parameter space by leveraging proposals created using Hamiltonian dynamics defined as a system of differential equations satisfying the law of conservation of energy  \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "22. **p1q22**. Skipping the \"Arbitrary deterministics\" and \"Arbitrary distributions\" sections, what is the difference between Stan and PyMC according to the \"Discussion\" section?\n",
    "\n",
    "    1. PyMC supports discrete variables\n",
    "    2. PyMC supports non-gradient based sampling algorithms like Metropolis-Hastings and Slice sampling\n",
    "    3. PyMC supports automatic transforms of constrained random variables \n",
    "    4. A and B only\n",
    "    \n",
    "<br>\n",
    "\n",
    "23. **p1q23**. What does the discussion state about variational inference relative to MCMC sampling?\n",
    "\n",
    "    1. It is a promising research area that may provide both better efficiency and generalizbility than MCMC samplilng\n",
    "    2. It was previously a state of the art methodology which has now been replaced by MCMC sampling in PyMC\n",
    "    3. It says that methods like ADVI do not require automatic differentiation like HMC which is computationally beneficial\n",
    "    4. It is more efficient but may have less generalizability than MCMC sampling\n",
    "    \n",
    "<br>\n",
    "\n",
    "24. **p1q24**. What is the next section in the introductory material (found after the references)?\n",
    "\n",
    "    1. Notebooks on core features\n",
    "    2. GLM: Linear regression\n",
    "    3. Model Comparision and Prior and Posterior Predictive Checks\n",
    "    4. PyMC and PyTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c4ca8",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 2 points (0.2 points each) [format: `str` either \"A\" or \"B\" or \"C\" or \"D\" based on the choices above]\n",
    "p1q10 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q11 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q12 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q13 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q14 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q15 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q16 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q17 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q18 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q19 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q20 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q21 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q22 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q23 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q24 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "# Uncomment the above and keep only either \"A\" or \"B\" or \"C\" or \"D\"\n",
    "\n",
    "# This cell will produce a runtime error until the variables `p1q10`-`p1q25` are assigned values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73598cbd",
   "metadata": {},
   "source": [
    "# Problem 2 (5 points)\n",
    "\n",
    "# IMPORTANT/CRITICAL: this assignment must be completed as TWO separate notebooks. \n",
    "\n",
    "# *DO NOT PUT PyMC OR ANY OTHER CODE IN THIS NOTEBOOK*\n",
    "## Add and execute your code in another notebook named `PyMC_intro.ipynb`\n",
    "\n",
    "### *PyMC is currently undergoing many updates and is (unfortunately) thus not currently compatible with MarkUs and the UofT JuypterHub resources; thus, for versioning and package availability compatibility you MUST complete this assignment with google colab or a local anaconda installation. For a local conda installation use the following envirionment creation and activation.*\n",
    "\n",
    "```\n",
    "conda create -c conda-forge -n pymc_env \"pymc>=4\"\n",
    "conda activate pymc_env\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "1. Go to https://www.pymc.io/welcome.html <!--https://docs.pymc.io/en/v3/-->\n",
    "2. Scroll down a little bit and click the \"API quickstart (if you do know Bayesian modeling)\" link, which will take you [here](https://www.pymc.io/projects/examples/en/latest/howto/api_quickstart.html)\n",
    "2. Complete the tutorial by copying and running the necessary code as cells into A DIFFERENT NOTEBOOK titled `PyMC_intro.ipynb`.\n",
    "3. Answer the following questions in this notebook\n",
    "\n",
    "# ONLY PROVIDE ABCD ANSWERS here: don't include PyMC code here\n",
    "\n",
    "***The following 25 questions are each worth 0.2 points.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca28e68",
   "metadata": {},
   "source": [
    "<!-- FIXED SO THIS IS NO LONGER NEEDED!\n",
    "Replace the following so that we use the `aesara` library that is currently stble on google colab rather than the in progress `pytensor` library. \n",
    "\n",
    "```python\n",
    "#import pytensor.tensor as pt\n",
    "import aesara\n",
    "import aesara.tensor as pt\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc362e",
   "metadata": {},
   "source": [
    "0. **p2q0**. What `keyword` specifies observed random variables in `PyMC`? \n",
    "\n",
    "    1. observed\n",
    "    2. observable\n",
    "    3. observation\n",
    "    4. obs_random_variable\n",
    "    \n",
    "<br>\n",
    "\n",
    "1. **p2q1**. What data types can be used to assign observed values to random variables in `PyMC`?\n",
    "\n",
    "    1. lists\n",
    "    2. numpy.ndarray\n",
    "    3. pytensor data structures\n",
    "    4. All of the above\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **p2q2**. When using overloaded operators like `+`, `*`, and `**`, or functions from `pm.math`, what's the difference between wrapping them inside of a `pm.Deterministic` function or not? \n",
    "\n",
    "     1. Using `pm.Deterministic` means the PyMC to keep track of the operations as a transformed varible\n",
    "     2. The overloaded operators and `pm.math` functions can only be used with `pm.Deterministic`\n",
    "     3. Using `pm.Deterministic` sets the result one time to a single fixed value for all future uses\n",
    "     4. There is no appreciable difference\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **p2q3**. For the code below\n",
    "\n",
    "    >```python\n",
    "    > with pm.Model() as model:\n",
    "    >     mu = pm.Normal(\"mu\", mu=0, sigma=1)\n",
    "    >     obs = pm.Normal(\"obs\", mu=mu, sigma=1, observed=rng.standard_normal(100))\n",
    "    \n",
    "    which of the following evalutes just the prior distribution? \n",
    "\n",
    "    1. `pm.logp(mu, 0).eval()`\n",
    "    2. `pm.logp(obs, 0).eval()`\n",
    "    3. `pm.logp(obs, dat).eval().sum()`\n",
    "    4. `model.compile_logp()({\"mu\": 0})`    \n",
    "\n",
    "<br>\n",
    "\n",
    "4. **p2q4**. Run the code below and then examine `model.value_vars` and `model.free_RVs`.  \n",
    "\n",
    "    >```python\n",
    "    > with pm.Model() as model:\n",
    "    >     x = pm.Normal(\"x\", mu=0, sigma=1)\n",
    "    >     y = pm.Gamma(\"y\", alpha=1, beta=1)\n",
    "    >     plus_2 = x + 2\n",
    "    >     summed = x + y\n",
    "    >     squared = x**2\n",
    "    >     sined = pm.math.sin(x)\n",
    "    >```\n",
    "\n",
    "    Which of the following is an explanation for why the former differs from the latter? \n",
    "\n",
    "    1. Discrete random variables need to be converted to continuous random variables for the HMC NUTS implementation to work well\n",
    "    2. The transformed $\\log(y)$ is unbounded and is used instead of $y$ \"under the hood\" so that \"out of bounds\" errors don't have to be checked\n",
    "    3. The log transformed random variable is not as numerically accurate as the untransformed variable\n",
    "    4. The $\\log(y)$ transform is explicitly defined in the PyMC code specification\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "5. **p2q5**. PyMC has a [transforms class](https://www.pymc.io/projects/docs/en/stable/api/distributions/transforms.html), which in particular includes the [chain method](https://www.pymc.io/projects/docs/en/stable/api/distributions/generated/pymc.distributions.transforms.Chain.html#pymc.distributions.transforms.Chain), which defines which of the following methods?\n",
    "\n",
    "    1. `Chain.backward # Invert the transformation`\n",
    "    2. `Chain.forward # Apply the transformation`\n",
    "    3. `Chain.log_jac_det # Construct the log of the absolute value of the Jacobian determinant`\n",
    "    4. All of the above\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "6. **p2q6**. To transform a random variable into another random variable in PyMC requires which of the following?\n",
    "\n",
    "    1. `Chain.backward # Invert the transformation`\n",
    "    2. `Chain.forward # Apply the transformation`\n",
    "    3. `Chain.log_jac_det # Construct the log of the absolute value of the Jacobian determinant`\n",
    "    4. All of the above\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "7. **p2q7**. To evaluate the density of a transformed random variable in terms of the random variable being transformed in PyMC requires which of the following? (***Hint***: what is required for defining the density of a transformed random variable in terms of the other variable, generally?)\n",
    "\n",
    "    1. `Chain.backward # Invert the transformation`\n",
    "    2. `Chain.forward # Apply the transformation`\n",
    "    3. `Chain.log_jac_det # Construct the log of the absolute value of the Jacobian determinant`\n",
    "    4. A and B\n",
    "    5. A and C\n",
    "    6. All of the above\n",
    "\n",
    "<br>\n",
    "\n",
    "8. **p2q8**. The change of variables transformation discussed above appears in [TensorFlow Probability](https://www.tensorflow.org/probability) as part of the [Bijector](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/Bijector) class. The `Chain.log_jac_det` in PyMC `Transform` class corresponds to the `inverse_log_det_jacobian` in TensorFlow `Bijection` class, as can be seen from the [source code](https://www.pymc.io/projects/docs/en/stable/_modules/pymc/distributions/transforms.html#Chain.log_jac_det) which accumulates `Chain.log_jac_det` in tandem with `transf.backward`.  What additional `log_det_jacobian` computation does the TensorFlow `Bijection` class include that the PyMC `Transform` class doesn't? \n",
    "\n",
    "    1. `backward_log_det_jacobian`\n",
    "    2. `forward_log_det_jacobian`\n",
    "    3. `inverse`\n",
    "    4. `backward`\n",
    "    \n",
    "<br>\n",
    "\n",
    "9. **p2q9**. Which of the following is the correct way to specify a 10 dimensional random variable?\n",
    "\n",
    "    1. `x = [pm.Normal(f\"x_{i}\", mu=0, sigma=1) for i in range(10)]`\n",
    "    2. `x = pm.Normal(\"x\", mu=0, sigma=1, shape=10)`\n",
    "    3. Either of the above are the correct specificiation\n",
    "    4. Neither of the above are the correct specificiation\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "10. **p2q10**. What is the output of the following code which uses the `.initial_point()` method based on the `initval` argument?\n",
    "\n",
    "    > ```python\n",
    "    > np.random.seed(10)\n",
    "    > with pm.Model():\n",
    "    >    x = pm.Normal(\"x\", mu=0, sigma=1, shape=5, initval=np.random.randn(5))\n",
    "    > model.initial_point()['x'].dot(model.initial_point()['x'].T)\n",
    "    > ```    \n",
    "\n",
    "    1. 5.14\n",
    "    2. -0.52 \n",
    "    3. 0.46\n",
    "    4. None of the above\n",
    "    \n",
    "\n",
    "<br>\n",
    "\n",
    "11. **p2q11**. Which of the following is a not parameter of the `pm.sample` function?  (***Hint***: see `pm.sample?`)\n",
    "\n",
    "    1. chains\n",
    "    2. observed\n",
    "    3. step\n",
    "    4. tune\n",
    "\n",
    "<br>\n",
    "\n",
    "12. **p2q12**. Which of the following is not a method available for the `step` parameter in PyMC? (***Hint***: see `pm.sample?`)\n",
    "\n",
    "    1. `\"binary_gibbs_metropolis\" # pm.BinaryGibbsMetropolis()`\n",
    "    2. `\"DEMetropolisZ\" # pm.DEMetropolisZ()`\n",
    "    3. `\"hmc\" # pm.HMC()`\n",
    "    4. `\"laplace_proposal\" # pm.LaplaceProposal()`\n",
    "\n",
    "<br>\n",
    "\n",
    "13. **p2q13**. Which of the following allows for visual inspect the MCMC chains and their marginal distributions?\n",
    "\n",
    "    1. Forest plots\n",
    "    2. Energy plots\n",
    "    3. Trace plots\n",
    "    4. `ess_bulk`, `ess_tail`, and `r_hat` diagnostic summaires\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "14. **p2q14**. HDI stand for \"highest density interval\" and is an interval in a posterior distribution containing a Bayesian credible interval, which is the Bayesian analog of a confidence interval.  What is the purpose of a forest plot?\n",
    "\n",
    "    1. To determine the number of independent Markov chains that should be run\n",
    "    2. To visualize and interpret predictions from random forest ensemble models\n",
    "    3. To characterize the degree of autocorrelation present in Markov chains\n",
    "    4. To see if samples disagree across chains, indicating lack of convergence\n",
    "\n",
    "<br>\n",
    "\n",
    "15. **p2q15**. Run the code below and select the answer which describes the difference between the plot now created and the output of the tutorial.\n",
    "\n",
    "    > ```python\n",
    "    > np.random.seed(15)\n",
    "    > with pm.Model() as model:\n",
    "    >     mu = pm.Normal(\"mu\", mu=0, sigma=1)\n",
    "    >     sd = pm.HalfNormal(\"sd\", sigma=1)\n",
    "    >     obs = pm.Normal(\"obs\", mu=mu, sigma=sd, observed=np.random.randn(100))\n",
    "    >     step1 = pm.Metropolis(vars=[mu])\n",
    "    >     step2 = pm.Slice(vars=[sd])\n",
    "    >     idata = pm.sample(10000, step=[step1, step2], cores=4)\n",
    "    > az.plot_trace(idata);    \n",
    "    > ```\n",
    "    \n",
    "    1. It does not differ in any notable or substantial way\n",
    "    2. The KDE is more \"jagged\" and less \"smooth\"\n",
    "    3. The middle 95% of the samples don't overlap with the middle 95% of the samples from the tutorial\n",
    "    4. There is a difference but it's just \"run to run variability\" and goes away for different seeds\n",
    "\n",
    "<br>\n",
    "\n",
    "16. **p2q16**. Run `az.plot_energy(idata);` and choose the option below that best describes the result. \n",
    "\n",
    "    1. The marginal energy and the energy transition are no longer unimodal\n",
    "    2. The marginal energy and the energy transition overlap as in the tutorial\n",
    "    3. The marginal energy and the energy transition do not overlap as well as in the tutorial\n",
    "    4. There is no plot due to `AttributeError: 'Dataset' object has no attribute 'energy'`\n",
    "\n",
    "<br> \n",
    "\n",
    "17. **p2q17**. What explains the answer to the previous question.\n",
    "\n",
    "    1. This is a Metropolis-Hastings and Slice sampler, not Hamiltonian Dynamics\n",
    "    2. The different sampling schemes result in different energy transition distributions\n",
    "    3. The models are different and thus have different marginal energy distributions\n",
    "    4. We don't have enough information to be able to say what the explanation is\n",
    "\n",
    "<br> \n",
    "\n",
    "18. **p2q18**. What benefit does the \"energy\" characterization available from NUTS provide?\n",
    "\n",
    "    <!-- *Notes: The \"No U-Turn\" sampler (NUTS) is a specific Hamiltonian Monte Carlo (HMC) algorithm implementation. The \"energy\" referred to in PyMC is from the Hamiltonian dynamics (discussed in the \"Symplectic Integration\" section from the first part of the class notes). HMC methods, e.g., NUTS, use Hamiltonian dynamics to move along probability contours, but proposals must also transition between different contour levels as well.* -->\n",
    "\n",
    "    1. It can be used to create Bayesian credible (HDI) intervals of specific model parameters\n",
    "    2. It's the best way assess convergence when the parameter dimension very small \n",
    "    3. It can be used to assess convergence when the parameter dimension very large\n",
    "    4. It's not useful because it's the log joint density which includes the auxiliary parameters\n",
    "    \n",
    "<br>     \n",
    "    \n",
    "19. **p2q19**.  What does `az.plot_energy(short_trace);` look like for the following code?\n",
    "\n",
    "    > ```python\n",
    "    > # https://docs.pymc.io/en/v3/pymc-examples/examples/diagnostics_and_criticism/Diagnosing_biased_Inference_with_Divergences.html\n",
    "    > # Data of the Eight Schools Model\n",
    "    > J = 8\n",
    "    > y = np.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0])\n",
    "    > sigma = np.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0])\n",
    "    > # tau = 25.\n",
    "    > SEED = [20100420, 20134234]\n",
    "    > with pm.Model() as Centered_eight:\n",
    "    >   mu = pm.Normal(\"mu\", mu=0, sigma=5)\n",
    "    >   tau = pm.HalfCauchy(\"tau\", beta=5)\n",
    "    >   theta = pm.Normal(\"theta\", mu=mu, sigma=tau, shape=J)\n",
    "    >   obs = pm.Normal(\"obs\", mu=theta, sigma=sigma, observed=y)\n",
    "    >\n",
    "    > with Centered_eight:\n",
    "    >   short_trace = pm.sample(600, chains=2, random_seed=SEED)\n",
    "    >\n",
    "    > az.plot_energy(short_trace);  \n",
    "    >```\n",
    "\n",
    "     1. It looks like the `az.plot_energy(short_trace);` plot in the tutorial\n",
    "     2. The energy transition distribution is narrower than the marginal energy distribution\n",
    "     3. The energy transition distribution is wider than the marginal energy distribution\n",
    "     4. There is no plot due to `AttributeError: 'Dataset' object has no attribute 'energy'`    \n",
    "     \n",
    "   \n",
    "<br> \n",
    "\n",
    "20. **p2q20**. What is the difference between `ADVI` and `fullrank_ADVI`?\n",
    "\n",
    "    1. One is gradient based an the other is not\n",
    "    2. The rank of the covariance matrix which they specify\n",
    "    3. One is based on normal distributions while the other is not\n",
    "    4. Independent normal specifications versus full covariance matrix specifications\n",
    "\n",
    "<br> \n",
    "\n",
    "21. **p1q21**. What model fitting techniques available in PyMC are demonstrated in the tutorial?\n",
    "\n",
    "    1. MCMC/HMC\n",
    "    2. MCMC/HMC, Variational Inference\n",
    "    3. MCMC/HMC, Variational Inference, SVGD using particles\n",
    "    4. MCMC/HMC, Variational Inference, SVGD using particles, Maximum Likelihood Estimation\n",
    "\n",
    "<br> \n",
    "\n",
    "22. **p1q22**. Run the code below, then select the option below best describing the result. \n",
    "    > ```python\n",
    "    > print(len(set(approx.sample(50000).posterior['x'][0].values.tolist())))\n",
    "    > with pm.Model() as model:\n",
    "    >     pm.NormalMixture(\"x\", w=[0.2, 0.8], mu=[-0.3, 0.5], sigma=[0.1, 0.1])\n",
    "    >     approx = pm.fit(method=pm.SVGD(n_particles=200, jitter=1.0))\n",
    "    > idata = approx.sample(150000)\n",
    "    > az.plot_dist(idata.posterior[\"x\"])\n",
    "    > ```\n",
    "\n",
    "    1. The KDE of `az.plot_dist(idata.posterior[\"x\"])` remains the same as it is in the tutorial\n",
    "    2. 150000 samples from 200 unique \"particles\" representing the distribution isn't \"smooth\"\n",
    "    3. The \"particle filtering\" method appears to be broken or is just not a useful method\n",
    "    4. There is no plot due to `AttributeError: 'Dataset' object has no attribute 'energy'`\n",
    "\n",
    "<br> \n",
    "\n",
    "23. **p1q23**. What are the \"posterior predictive\" lines in the \"Posterior Predictive Sampling\" section?\n",
    "\n",
    "    1. KDEs of samples of data from models which each correspond to a draw from the posterior distribution\n",
    "    2. The KDE of the original sample data used to create the posterior distribution\n",
    "    3. The average of the \"posterior predictive\" KDEs \n",
    "    4. The actual mean from which the data was produced\n",
    "\n",
    "<br> \n",
    "\n",
    "24. **p1q24**. Run the code below and select which of the options best explain the difference in the output compared to the tutorial.\n",
    "\n",
    "    >```python\n",
    "    > x = rng.standard_normal(100)\n",
    "    > y = x > 0\n",
    "    > coords = {\"idx\": np.arange(100)}\n",
    "    > with pm.Model() as model:\n",
    "    >   x_obs = pm.MutableData(\"x_obs\", x, dims=\"idx\")\n",
    "    >   y_obs = pm.MutableData(\"y_obs\", y, dims=\"idx\")\n",
    "    >   coeff = pm.Normal(\"x\", mu=0, sigma=1)\n",
    "    >   logistic = pm.math.sigmoid(coeff * x_obs)\n",
    "    >   pm.Bernoulli(\"obs\", p=logistic, observed=y_obs, dims=\"idx\")\n",
    "    >   idata = pm.sample()\n",
    "    > with model:\n",
    "    >   pm.set_data({\"x_obs\": [-0.01, 0., 0.01], \"y_obs\": [0, 0, 0],},\n",
    "    >               coords={\"idx\": [1004, 1005, 1006]},)\n",
    "    > az.plot_trace(idata)\n",
    "    > idata.extend(pm.sample_posterior_predictive(idata)) \n",
    "    > # https://discourse.pymc.io/t/calling-pm-set-data-multiple-times/11306/4\n",
    "    >```\n",
    "\n",
    "    1. The model is not strongly predictive of $y=\\text{sign}(x)$ for $|x|=0.01$.\n",
    "    2. Shifting the posterior mass of `coeff`  to the right by a large amount would improve $y=\\text{sign}(x)$ prediction  \n",
    "    3. The `coeff` doesn't simply optimize `pm.math.sigmoid(coeff * x_obs)` beause the posterior must also reflect the `pm.Normal(\"x\", mu=0, sigma=1)` prior\n",
    "    4. All of the above\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1a7e3",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 5 points (0.2 points each) [format: `str` either \"A\" or \"B\" or \"C\" or \"D\" based on the choices above]\n",
    "p2q0 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q1 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q2 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q3 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q4 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q5 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q6 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q7 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q8 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q9 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q10 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q11 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q12 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q13 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q14 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q15 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q16 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q17 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q18 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q19 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q20 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q21 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q22 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q23 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q24 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "# Uncomment the above and keep only either \"A\" or \"B\" or \"C\" or \"D\"\n",
    "\n",
    "# This cell will produce a runtime error until the variables `p2q0`-`p2q24` are assigned values"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
